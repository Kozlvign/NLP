{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de150eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 15.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\igork\\env\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\igork\\env\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\igork\\env\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\igork\\env\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece113b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\igork\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\igork\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4498ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = brown.tagged_sents(tagset='universal')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941a0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for sentence_tag_pairs in corpus:\n",
    "    tokens = []\n",
    "    target = []\n",
    "    for token, tag in sentence_tag_pairs:\n",
    "        tokens.append(token)\n",
    "        target.append(tag)\n",
    "    inputs.append(tokens)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051578da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    for x, y in zip(inputs, targets):\n",
    "        j = {'inputs': x, 'targets': y}\n",
    "        s = json.dumps(j)\n",
    "        f.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec78914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d12014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1d0c5733e7e27340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/igork/.cache/huggingface/datasets/json/default-1d0c5733e7e27340/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d475c8ad7c44c4f937606056de6fbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc77857e86949d4bad9fa0fa13cb89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/igork/.cache/huggingface/datasets/json/default-1d0c5733e7e27340/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b54cff76f21464e8e58db3caca0784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files='data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c392ac31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 57340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a79f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'targets'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small = data['train'].shuffle(seed=42).select(range(20_000))\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a5ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = small.train_test_split(seed=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a3bde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': ['Ulyate',\n",
       "  'and',\n",
       "  'Kearton',\n",
       "  'climbed',\n",
       "  'on',\n",
       "  'toward',\n",
       "  'the',\n",
       "  'sound',\n",
       "  'of',\n",
       "  'the',\n",
       "  'barking',\n",
       "  'of',\n",
       "  'the',\n",
       "  'dogs',\n",
       "  'and',\n",
       "  'the',\n",
       "  'sporadic',\n",
       "  'roaring',\n",
       "  'of',\n",
       "  'the',\n",
       "  'lion',\n",
       "  ',',\n",
       "  'till',\n",
       "  'they',\n",
       "  'came',\n",
       "  ',',\n",
       "  'out',\n",
       "  'of',\n",
       "  'breath',\n",
       "  ',',\n",
       "  'to',\n",
       "  'the',\n",
       "  'crest',\n",
       "  ',',\n",
       "  'and',\n",
       "  'peering',\n",
       "  'through',\n",
       "  'the',\n",
       "  'branches',\n",
       "  'of',\n",
       "  'a',\n",
       "  'bush',\n",
       "  ',',\n",
       "  'this',\n",
       "  'is',\n",
       "  'what',\n",
       "  'Ulyate',\n",
       "  'saw',\n",
       "  ':',\n",
       "  'Jones',\n",
       "  'who',\n",
       "  'had',\n",
       "  'apparently',\n",
       "  '(',\n",
       "  'and',\n",
       "  'actually',\n",
       "  'had',\n",
       "  ')',\n",
       "  'ridden',\n",
       "  'up',\n",
       "  'the',\n",
       "  'nearly',\n",
       "  'impassable',\n",
       "  'hillside',\n",
       "  ',',\n",
       "  'sitting',\n",
       "  'calmly',\n",
       "  'on',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'within',\n",
       "  'forty',\n",
       "  'feet',\n",
       "  'of',\n",
       "  'a',\n",
       "  'full-grown',\n",
       "  'young',\n",
       "  'lioness',\n",
       "  ',',\n",
       "  'who',\n",
       "  'was',\n",
       "  'crouched',\n",
       "  'on',\n",
       "  'a',\n",
       "  'flat',\n",
       "  'rock',\n",
       "  'and',\n",
       "  'seemed',\n",
       "  'just',\n",
       "  'about',\n",
       "  'to',\n",
       "  'charge',\n",
       "  'him',\n",
       "  ',',\n",
       "  'while',\n",
       "  'the',\n",
       "  'dogs',\n",
       "  'whirled',\n",
       "  'around',\n",
       "  'her',\n",
       "  '.'],\n",
       " 'targets': ['NOUN',\n",
       "  'CONJ',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'PRT',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'CONJ',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'PRON',\n",
       "  'VERB',\n",
       "  '.',\n",
       "  'PRT',\n",
       "  'ADP',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'CONJ',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'DET',\n",
       "  'VERB',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  '.',\n",
       "  'NOUN',\n",
       "  'PRON',\n",
       "  'VERB',\n",
       "  'ADV',\n",
       "  '.',\n",
       "  'CONJ',\n",
       "  'ADV',\n",
       "  'VERB',\n",
       "  '.',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'ADV',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'VERB',\n",
       "  'ADV',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'NUM',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'PRON',\n",
       "  'VERB',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'CONJ',\n",
       "  'VERB',\n",
       "  'ADV',\n",
       "  'ADV',\n",
       "  'PRT',\n",
       "  'VERB',\n",
       "  'PRON',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'PRON',\n",
       "  '.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309a591f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'targets': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b41b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_set = set()\n",
    "for target in targets:\n",
    "    target_set = target_set.union(target)\n",
    "target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1aaad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = list(target_set)\n",
    "id2label = {k: v for k, v in enumerate(target_list)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70941a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bbb169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 158, 25928, 1566, 1105, 26835, 9349, 1320, 5998, 1113, 1755, 1103, 1839, 1104, 1103, 26635, 1104, 1103, 6363, 1105, 1103, 188, 27695, 23041, 1104, 1103, 11160, 117, 6174, 1152, 1338, 117, 1149, 1104, 2184, 117, 1106, 1103, 13468, 117, 1105, 19205, 1194, 1103, 5020, 1104, 170, 13771, 117, 1142, 1110, 1184, 158, 25928, 1566, 1486, 131, 2690, 1150, 1125, 4547, 113, 1105, 2140, 1125, 114, 17698, 1146, 1103, 2212, 24034, 11192, 1895, 25068, 117, 2807, 13285, 1113, 1117, 3241, 1439, 5808, 1623, 1104, 170, 1554, 118, 4215, 1685, 11160, 5800, 117, 1150, 1108, 15062, 1113, 170, 3596, 2067, 1105, 1882, 1198, 1164, 1106, 2965, 1140, 117, 1229, 1103, 6363, 18370, 1213, 1123, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "t = tokenizer(data[\"train\"][idx][\"inputs\"], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a080dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa7ee93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'U',\n",
       " '##lya',\n",
       " '##te',\n",
       " 'and',\n",
       " 'Ke',\n",
       " '##art',\n",
       " '##on',\n",
       " 'climbed',\n",
       " 'on',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'of',\n",
       " 'the',\n",
       " 'barking',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dogs',\n",
       " 'and',\n",
       " 'the',\n",
       " 's',\n",
       " '##poradic',\n",
       " 'roaring',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lion',\n",
       " ',',\n",
       " 'till',\n",
       " 'they',\n",
       " 'came',\n",
       " ',',\n",
       " 'out',\n",
       " 'of',\n",
       " 'breath',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'crest',\n",
       " ',',\n",
       " 'and',\n",
       " 'peering',\n",
       " 'through',\n",
       " 'the',\n",
       " 'branches',\n",
       " 'of',\n",
       " 'a',\n",
       " 'bush',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'what',\n",
       " 'U',\n",
       " '##lya',\n",
       " '##te',\n",
       " 'saw',\n",
       " ':',\n",
       " 'Jones',\n",
       " 'who',\n",
       " 'had',\n",
       " 'apparently',\n",
       " '(',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'had',\n",
       " ')',\n",
       " 'ridden',\n",
       " 'up',\n",
       " 'the',\n",
       " 'nearly',\n",
       " 'imp',\n",
       " '##ass',\n",
       " '##able',\n",
       " 'hillside',\n",
       " ',',\n",
       " 'sitting',\n",
       " 'calmly',\n",
       " 'on',\n",
       " 'his',\n",
       " 'horse',\n",
       " 'within',\n",
       " 'forty',\n",
       " 'feet',\n",
       " 'of',\n",
       " 'a',\n",
       " 'full',\n",
       " '-',\n",
       " 'grown',\n",
       " 'young',\n",
       " 'lion',\n",
       " '##ess',\n",
       " ',',\n",
       " 'who',\n",
       " 'was',\n",
       " 'crouched',\n",
       " 'on',\n",
       " 'a',\n",
       " 'flat',\n",
       " 'rock',\n",
       " 'and',\n",
       " 'seemed',\n",
       " 'just',\n",
       " 'about',\n",
       " 'to',\n",
       " 'charge',\n",
       " 'him',\n",
       " ',',\n",
       " 'while',\n",
       " 'the',\n",
       " 'dogs',\n",
       " 'whirled',\n",
       " 'around',\n",
       " 'her',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3a6b297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d965ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100\n",
    "        else:\n",
    "            label = label2id[labels[word]]\n",
    "        aligned_labels.append(label)\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547069c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 11,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 11,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 11,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 11,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 11,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 11,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 11,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 11,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 5,\n",
       " 2,\n",
       " 11,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 11,\n",
       " -100]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try our function\n",
    "labels = data['train'][idx]['targets']\n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d87d709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "U\tNOUN\n",
      "##lya\tNOUN\n",
      "##te\tNOUN\n",
      "and\tCONJ\n",
      "Ke\tNOUN\n",
      "##art\tNOUN\n",
      "##on\tNOUN\n",
      "climbed\tVERB\n",
      "on\tPRT\n",
      "toward\tADP\n",
      "the\tDET\n",
      "sound\tNOUN\n",
      "of\tADP\n",
      "the\tDET\n",
      "barking\tNOUN\n",
      "of\tADP\n",
      "the\tDET\n",
      "dogs\tNOUN\n",
      "and\tCONJ\n",
      "the\tDET\n",
      "s\tADJ\n",
      "##poradic\tADJ\n",
      "roaring\tNOUN\n",
      "of\tADP\n",
      "the\tDET\n",
      "lion\tNOUN\n",
      ",\t.\n",
      "till\tADP\n",
      "they\tPRON\n",
      "came\tVERB\n",
      ",\t.\n",
      "out\tPRT\n",
      "of\tADP\n",
      "breath\tNOUN\n",
      ",\t.\n",
      "to\tADP\n",
      "the\tDET\n",
      "crest\tNOUN\n",
      ",\t.\n",
      "and\tCONJ\n",
      "peering\tVERB\n",
      "through\tADP\n",
      "the\tDET\n",
      "branches\tNOUN\n",
      "of\tADP\n",
      "a\tDET\n",
      "bush\tNOUN\n",
      ",\t.\n",
      "this\tDET\n",
      "is\tVERB\n",
      "what\tDET\n",
      "U\tNOUN\n",
      "##lya\tNOUN\n",
      "##te\tNOUN\n",
      "saw\tVERB\n",
      ":\t.\n",
      "Jones\tNOUN\n",
      "who\tPRON\n",
      "had\tVERB\n",
      "apparently\tADV\n",
      "(\t.\n",
      "and\tCONJ\n",
      "actually\tADV\n",
      "had\tVERB\n",
      ")\t.\n",
      "ridden\tVERB\n",
      "up\tADP\n",
      "the\tDET\n",
      "nearly\tADV\n",
      "imp\tADJ\n",
      "##ass\tADJ\n",
      "##able\tADJ\n",
      "hillside\tNOUN\n",
      ",\t.\n",
      "sitting\tVERB\n",
      "calmly\tADV\n",
      "on\tADP\n",
      "his\tDET\n",
      "horse\tNOUN\n",
      "within\tADP\n",
      "forty\tNUM\n",
      "feet\tNOUN\n",
      "of\tADP\n",
      "a\tDET\n",
      "full\tADJ\n",
      "-\tADJ\n",
      "grown\tADJ\n",
      "young\tADJ\n",
      "lion\tNOUN\n",
      "##ess\tNOUN\n",
      ",\t.\n",
      "who\tPRON\n",
      "was\tVERB\n",
      "crouched\tVERB\n",
      "on\tADP\n",
      "a\tDET\n",
      "flat\tADJ\n",
      "rock\tNOUN\n",
      "and\tCONJ\n",
      "seemed\tVERB\n",
      "just\tADV\n",
      "about\tADV\n",
      "to\tPRT\n",
      "charge\tVERB\n",
      "him\tPRON\n",
      ",\t.\n",
      "while\tADP\n",
      "the\tDET\n",
      "dogs\tNOUN\n",
      "whirled\tVERB\n",
      "around\tADP\n",
      "her\tPRON\n",
      ".\t.\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "# print out aligned labels along with its correspondent tokenized input in correspondent order\n",
    "aligned_labels = [id2label[i] if i >= 0 else None for i in aligned_targets]\n",
    "for x, y in zip(t.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f938b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['inputs'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    labels_batch = batch['targets'] # original targets\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d624d1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inputs', 'targets']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a825f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dd62c4d4fe41f8ab848cd75be6f4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962a3a22f9984f03997c7119a42c143b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=data[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d882dd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "731e3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "492ba770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f72226a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    labels_jagged = [[t for t in label if t != -100] for label in labels]\n",
    "    preds_jagged = [[p for p, t in zip(ps, ts) if t != -100] for ps, ts in zip(preds, labels)]\n",
    "    labels_flat = flatten(labels_jagged)\n",
    "    preds_flat = flatten(preds_jagged)\n",
    "    acc = accuracy_score(labels_flat, preds_flat)\n",
    "    f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef48e53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6, 'accuracy': 0.8}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [[-100, 0, 0, 1, 2, 1, -100]]\n",
    "logits = np.array([[\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "]])\n",
    "compute_metrics((logits, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf046f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ce832b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    'distilbert-finetuned-ner',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6065ecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igork\\env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3750\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 07:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.058196</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.982979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.054347</td>\n",
       "      <td>0.949467</td>\n",
       "      <td>0.985293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuned-ner\\checkpoint-1875\n",
      "Configuration saved in distilbert-finetuned-ner\\checkpoint-1875\\config.json\n",
      "Model weights saved in distilbert-finetuned-ner\\checkpoint-1875\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuned-ner\\checkpoint-1875\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuned-ner\\checkpoint-1875\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuned-ner\\checkpoint-3750\n",
      "Configuration saved in distilbert-finetuned-ner\\checkpoint-3750\\config.json\n",
      "Model weights saved in distilbert-finetuned-ner\\checkpoint-3750\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuned-ner\\checkpoint-3750\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuned-ner\\checkpoint-3750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.06698279075622558, metrics={'train_runtime': 462.869, 'train_samples_per_second': 64.813, 'train_steps_per_second': 8.102, 'total_flos': 386431490254464.0, 'train_loss': 0.06698279075622558, 'epoch': 2.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ed4b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pos_model\n",
      "Configuration saved in pos_model\\config.json\n",
      "Model weights saved in pos_model\\pytorch_model.bin\n",
      "tokenizer config file saved in pos_model\\tokenizer_config.json\n",
      "Special tokens file saved in pos_model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('pos_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ee391bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pos_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"pos_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADV\",\n",
      "    \"1\": \"ADJ\",\n",
      "    \"2\": \"PRON\",\n",
      "    \"3\": \"X\",\n",
      "    \"4\": \"NUM\",\n",
      "    \"5\": \"VERB\",\n",
      "    \"6\": \"DET\",\n",
      "    \"7\": \"NOUN\",\n",
      "    \"8\": \"CONJ\",\n",
      "    \"9\": \"ADP\",\n",
      "    \"10\": \"PRT\",\n",
      "    \"11\": \".\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 11,\n",
      "    \"ADJ\": 1,\n",
      "    \"ADP\": 9,\n",
      "    \"ADV\": 0,\n",
      "    \"CONJ\": 8,\n",
      "    \"DET\": 6,\n",
      "    \"NOUN\": 7,\n",
      "    \"NUM\": 4,\n",
      "    \"PRON\": 2,\n",
      "    \"PRT\": 10,\n",
      "    \"VERB\": 5,\n",
      "    \"X\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file pos_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"pos_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADV\",\n",
      "    \"1\": \"ADJ\",\n",
      "    \"2\": \"PRON\",\n",
      "    \"3\": \"X\",\n",
      "    \"4\": \"NUM\",\n",
      "    \"5\": \"VERB\",\n",
      "    \"6\": \"DET\",\n",
      "    \"7\": \"NOUN\",\n",
      "    \"8\": \"CONJ\",\n",
      "    \"9\": \"ADP\",\n",
      "    \"10\": \"PRT\",\n",
      "    \"11\": \".\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 11,\n",
      "    \"ADJ\": 1,\n",
      "    \"ADP\": 9,\n",
      "    \"ADV\": 0,\n",
      "    \"CONJ\": 8,\n",
      "    \"DET\": 6,\n",
      "    \"NOUN\": 7,\n",
      "    \"NUM\": 4,\n",
      "    \"PRON\": 2,\n",
      "    \"PRT\": 10,\n",
      "    \"VERB\": 5,\n",
      "    \"X\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pos_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at pos_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    'token-classification',\n",
    "    model='pos_model',\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f72de03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'NOUN',\n",
       "  'score': 0.9995951,\n",
       "  'index': 1,\n",
       "  'word': 'Bill',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9997392,\n",
       "  'index': 2,\n",
       "  'word': 'Gates',\n",
       "  'start': 5,\n",
       "  'end': 10},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.9997534,\n",
       "  'index': 3,\n",
       "  'word': 'was',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity': 'DET',\n",
       "  'score': 0.99990046,\n",
       "  'index': 4,\n",
       "  'word': 'the',\n",
       "  'start': 15,\n",
       "  'end': 18},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99902403,\n",
       "  'index': 5,\n",
       "  'word': 'CEO',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9997826,\n",
       "  'index': 6,\n",
       "  'word': 'of',\n",
       "  'start': 23,\n",
       "  'end': 25},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99966145,\n",
       "  'index': 7,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9995982,\n",
       "  'index': 8,\n",
       "  'word': 'in',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99977034,\n",
       "  'index': 9,\n",
       "  'word': 'Seattle',\n",
       "  'start': 39,\n",
       "  'end': 46},\n",
       " {'entity': '.',\n",
       "  'score': 0.99987936,\n",
       "  'index': 10,\n",
       "  'word': ',',\n",
       "  'start': 46,\n",
       "  'end': 47},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99976677,\n",
       "  'index': 11,\n",
       "  'word': 'Washington',\n",
       "  'start': 48,\n",
       "  'end': 58},\n",
       " {'entity': '.',\n",
       "  'score': 0.9998753,\n",
       "  'index': 12,\n",
       "  'word': '.',\n",
       "  'start': 58,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Bill Gates was the CEO of Microsoft in Seattle, Washington.\"\n",
    "pipe(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
